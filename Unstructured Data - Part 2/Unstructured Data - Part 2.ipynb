{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Data - Continued\n",
    "\n",
    "Picking up where we left off - in this lab we'll use the same book data set that we web scraped. We'll pick a large genre, and see if we can identify some common words for the genre. We'll then see if we can distinguish genre's from each another.\n",
    "\n",
    "This brings us to the fundamental problem of both data engineering and data science work with unstructured data. That is, in general, computers understand numbers better than words. This sounds obvious, but it is the fundamental problem that has to be solved when extracting information from unstructured data.\n",
    "\n",
    "This lab will require a bit of handholding, as there are some complex concepts we're going jump into.\n",
    "\n",
    "Let's get started. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas, nltk, from nltk.tokenize.api import TokenizerI, matplotlib (inline), numpy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import the model_selection class from scikit\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>Ã‚Â£51.77</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>Three</td>\n",
       "      <td>a-light-in-the-attic_1000/index.html</td>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Ã‚Â£53.74</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>One</td>\n",
       "      <td>tipping-the-velvet_999/index.html</td>\n",
       "      <td>Historical Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Ã‚Â£50.10</td>\n",
       "      <td>Dans une France assez proche de la nÃƒÂ´tre, un ...</td>\n",
       "      <td>One</td>\n",
       "      <td>soumission_998/index.html</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>Ã‚Â£47.82</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>Four</td>\n",
       "      <td>sharp-objects_997/index.html</td>\n",
       "      <td>Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>Ã‚Â£54.23</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>Five</td>\n",
       "      <td>sapiens-a-brief-history-of-humankind_996/index...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title    Price  \\\n",
       "0                   A Light in the Attic  Ã‚Â£51.77   \n",
       "1                     Tipping the Velvet  Ã‚Â£53.74   \n",
       "2                             Soumission  Ã‚Â£50.10   \n",
       "3                          Sharp Objects  Ã‚Â£47.82   \n",
       "4  Sapiens: A Brief History of Humankind  Ã‚Â£54.23   \n",
       "\n",
       "                                         Description Rating  \\\n",
       "0  It's hard to imagine a world without A Light i...  Three   \n",
       "1  \"Erotic and absorbing...Written with starling ...    One   \n",
       "2  Dans une France assez proche de la nÃƒÂ´tre, un ...    One   \n",
       "3  WICKED above her hipbone, GIRL across her hear...   Four   \n",
       "4  From a renowned historian comes a groundbreaki...   Five   \n",
       "\n",
       "                                                Link               Genre  \n",
       "0               a-light-in-the-attic_1000/index.html              Poetry  \n",
       "1                  tipping-the-velvet_999/index.html  Historical Fiction  \n",
       "2                          soumission_998/index.html             Fiction  \n",
       "3                       sharp-objects_997/index.html             Mystery  \n",
       "4  sapiens-a-brief-history-of-humankind_996/index...             History  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the book data\n",
    "\n",
    "data = pd.read_csv('scraped_books.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Default           152\n",
       "Nonfiction        110\n",
       "Sequential Art     75\n",
       "Add a comment      67\n",
       "Fiction            65\n",
       "Young Adult        54\n",
       "Fantasy            48\n",
       "Romance            35\n",
       "Mystery            32\n",
       "Food and Drink     30\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the top 10 genres\n",
    "\n",
    "data.Genre.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the top few genres are fairly broad, so let's pick a genre that likely has more descriptive keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Unicorn Tracks</td>\n",
       "      <td>Ã‚Â£18.78</td>\n",
       "      <td>After a savage attack drives her from her home...</td>\n",
       "      <td>Three</td>\n",
       "      <td>unicorn-tracks_951/index.html</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Saga, Volume 6 (Saga (Collected Editions) #6)</td>\n",
       "      <td>Ã‚Â£25.02</td>\n",
       "      <td>After a dramatic time jump, the three-time Eis...</td>\n",
       "      <td>Three</td>\n",
       "      <td>saga-volume-6-saga-collected-editions-6_924/in...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Princess Between Worlds (Wide-Awake Princess #5)</td>\n",
       "      <td>Ã‚Â£13.34</td>\n",
       "      <td>Just as Annie and Liam are busy making plans t...</td>\n",
       "      <td>Five</td>\n",
       "      <td>princess-between-worlds-wide-awake-princess-5_...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Masks and Shadows</td>\n",
       "      <td>Ã‚Â£56.40</td>\n",
       "      <td>The year is 1779, and Carlo Morelli, the most ...</td>\n",
       "      <td>Two</td>\n",
       "      <td>masks-and-shadows_909/index.html</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Crown of Midnight (Throne of Glass #2)</td>\n",
       "      <td>Ã‚Â£43.29</td>\n",
       "      <td>\"A line that should never be crossed is about ...</td>\n",
       "      <td>Three</td>\n",
       "      <td>crown-of-midnight-throne-of-glass-2_888/index....</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title    Price  \\\n",
       "49                                     Unicorn Tracks  Ã‚Â£18.78   \n",
       "76      Saga, Volume 6 (Saga (Collected Editions) #6)  Ã‚Â£25.02   \n",
       "81   Princess Between Worlds (Wide-Awake Princess #5)  Ã‚Â£13.34   \n",
       "91                                  Masks and Shadows  Ã‚Â£56.40   \n",
       "112            Crown of Midnight (Throne of Glass #2)  Ã‚Â£43.29   \n",
       "\n",
       "                                           Description Rating  \\\n",
       "49   After a savage attack drives her from her home...  Three   \n",
       "76   After a dramatic time jump, the three-time Eis...  Three   \n",
       "81   Just as Annie and Liam are busy making plans t...   Five   \n",
       "91   The year is 1779, and Carlo Morelli, the most ...    Two   \n",
       "112  \"A line that should never be crossed is about ...  Three   \n",
       "\n",
       "                                                  Link    Genre  \n",
       "49                       unicorn-tracks_951/index.html  Fantasy  \n",
       "76   saga-volume-6-saga-collected-editions-6_924/in...  Fantasy  \n",
       "81   princess-between-worlds-wide-awake-princess-5_...  Fantasy  \n",
       "91                    masks-and-shadows_909/index.html  Fantasy  \n",
       "112  crown-of-midnight-throne-of-glass-2_888/index....  Fantasy  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a slice of the dataframe that is only books in the Fantasy genre and show the first 5 rows of it\n",
    "\n",
    "fantasy = data[data['Genre'] == 'Fantasy']\n",
    "fantasy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dipping our toes in. . .\n",
    "\n",
    "Now let's split this into a training and test set. The idea here is that we want to find common words in 70% of the Fantasy book descriptions. Then we'll see if we can accurately predict the other 30% of the books. There is a very easy way of doing this using scikit-learn's [*sklearn.model_selection.train_test_split()*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. To be totally clear, this is way overkill for this application, but it's a cool way to start using scikit, so let's do it. ðŸ˜‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataframe into a train (70%) and test (30%) set\n",
    "\n",
    "train, test = model_selection.train_test_split(fantasy, test_size=0.3, train_size=0.7, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the len of your train set to confirm you split properly\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the len of your test set to confirm you split properly\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a tokenization function that you can use apply to each row. It should tokenize the given row's description, remove stopwords, create a FreqDist for the tokens, remove anything that is < 1 character, and then return the top 5 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopwords and set them to use the english subset\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the tokenization function \n",
    "\n",
    "def common_word_getter(row):\n",
    "    words = nltk.word_tokenize(row.Description)\n",
    "    frequency = nltk.FreqDist(words)\n",
    "    frequency = [(w, f) for (w, f) in frequency.items() if w.lower() not in stopwords]\n",
    "    frequency = [(w, f) for (w, f) in frequency if len(w) > 1]\n",
    "    frequency.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    most_common = frequency[:5]\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of the most common words per book by iterating through the training set and applying your function\n",
    "\n",
    "common_list = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    common_list.extend([i[0] for i in common_word_getter(row)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['since',\n",
       " 'months',\n",
       " 'London',\n",
       " 'four',\n",
       " 'stone',\n",
       " 'Peter',\n",
       " \"'s\",\n",
       " 'Probationary',\n",
       " 'Constable',\n",
       " 'Grant']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the top of the common list\n",
    "\n",
    "common_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annnnd now we jump straight into the deep end ðŸ¤¯\n",
    "\n",
    "Up until this point, we've tokenized bits of text manually, and manually sorted, removed stopwords, etc. The hope here is to build some intuition around the steps required to work with text. Now, we're going to introduce some industry standard tools that do many of these steps together. We'll still go step by step, but these tools will allow us to abstract from some of the 'manual-ness' we've experienced thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate an instance of the CountVectorizer() class\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is really cool. We'll run it on our Description column, and use it to vectorize each piece of text. The vectorizing here is extremely simple, and is the most basic way of making the 'words to numbers' jump we discussed above.\n",
    "\n",
    "Essentially, you take every word in the piece of text you're analyzing and replace it with a 1. Then, for each additional instance of the same word, you add 1. We'll go step by step to show what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=10,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the stop_words argument equal to the stopwords we defined before, and set max_features to 10\n",
    "\n",
    "vect = CountVectorizer(stop_words=stopwords, max_features=10)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 107 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use vect.fit_transform() on the Description column in the training set to find the vectors of the training data\n",
    "\n",
    "train_vectors = vect.fit_transform(train.Description)\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'find',\n",
       " 'four',\n",
       " 'life',\n",
       " 'new',\n",
       " 'one',\n",
       " 'power',\n",
       " 'series',\n",
       " 'seven',\n",
       " 'world']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the feature names that sklearn found\n",
    "\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector is what's called a 'sparse matrix'. This is a numpy data type for storing large, sparse arrays. The dimensions of the vector are:\n",
    "\n",
    "- rows = # of samples vectorized\n",
    "- columns = # of features\n",
    "\n",
    "Now - here's the really mindblowing part :)\n",
    "\n",
    "Now that we've defined this 'vect' class, it will 'remember' it's vocabulary the next time we call it. This is because of the object oriented concept called inheritance. [This](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65) Medium post does a reasonable job at explaining the concept (as applies to sklearn specifically). \n",
    "\n",
    "For our purposes, it means that once we've instantiated the class on the training set, we can call it on the test set and it will remember the common words from the training set. This can be a bit confusing, so definitely do some reading on this before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 39 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the above on the test set\n",
    "\n",
    "test_vectors = vect.transform(test.Description)\n",
    "test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So now we have two matrices:\n",
    "\n",
    "- The train vector has the vectorization of the ten most common words throughout the corpus\n",
    "- The test vector has the vectorization of each of those ten words in the test set \n",
    "\n",
    "Now, if our goal is to use these training vectors to predict the genre of our test set, we need some way of comparing these vectors against each other. There are many ways to do this, so we'll start with using the cosine similarity. If you're really interested in understanding what's going on under the hood, read up on it [here.](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "At a high level, what we need to do a few things:\n",
    "\n",
    "- First, we need to find some measure of 'averageness' across our training vectors. This wil be a single vector that represents the average presence of each term across the training corpus.\n",
    "- Second, we need to score each of the test vectors against this 'average' vector\n",
    "- Third, we should look at those scores and see if there is any discernible pattern in them\n",
    "- Finally, the real test of 'predictiveness' will be to shuffle in some other genres with out test set, score them all against the average vector, and see if we can accurately distinguish the fantasy books from the rest of the data.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.75757576, 0.57575758, 0.54545455, 0.6969697 , 1.12121212,\n",
       "         0.96969697, 0.57575758, 0.90909091, 0.60606061, 0.93939394]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the average vector in the training set\n",
    "\n",
    "average_vector = train_vectors.mean(axis=0)\n",
    "average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score the test vectors using their cosine distance from the average vector\n",
    "\n",
    "scores = cosine_distances(X=average_vector, Y=test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53433771, 0.34467012, 0.7705006 , 0.52428744, 0.398394  ,\n",
       "        0.41920634, 0.6134747 , 0.6134747 , 0.24644118, 0.22091279,\n",
       "        0.56605469, 0.62555361, 0.48753501, 0.6134747 , 0.54159191]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the scores\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram the scores\n",
    "\n",
    "plt.hist(scores[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly shuffle in 50 rows from the other genres to the test set, and re-run the same prediction steps we ran above\n",
    "#feel free to also exclude the 'Add a comment' and 'Default' genres, as we don't know what their true genre is\n",
    "\n",
    "out_of_genre = test.append(data[data['Genre']!='Fantasy'].sample(50))\n",
    "out_of_genre = out_of_genre[out_of_genre['Genre']!='Default']\n",
    "out_of_genre = out_of_genre[out_of_genre['Genre']!='Add a comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<48x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 95 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the shuffled data through the vect class\n",
    "\n",
    "out_of_genre_vects = vect.transform(out_of_genre.Description)\n",
    "out_of_genre_vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score these out of genre vectors against the average vector we created above\n",
    "\n",
    "out_of_genre_scores = cosine_distances(average_vector, out_of_genre_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53433771, 0.34467012, 0.7705006 , 0.52428744, 0.398394  ,\n",
       "        0.41920634, 0.6134747 , 0.6134747 , 0.24644118, 0.22091279,\n",
       "        0.56605469, 0.62555361, 0.48753501, 0.6134747 , 0.54159191,\n",
       "        0.54732259, 0.69802711, 0.43631727, 1.        , 1.        ,\n",
       "        0.56783859, 1.        , 0.46063576, 0.55308012, 0.4761469 ,\n",
       "        0.42774743, 1.        , 1.        , 0.21712902, 0.62555361,\n",
       "        0.51923488, 0.40693701, 1.        , 0.56096126, 0.61421146,\n",
       "        0.72218494, 0.57865529, 0.40529782, 0.55164601, 0.48753501,\n",
       "        0.62187013, 0.55308012, 0.72218494, 0.43512502, 0.59511302,\n",
       "        0.57865529, 0.55308012, 0.46191176]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the out of genre scores\n",
    "\n",
    "out_of_genre_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANyUlEQVR4nO3df6xk9V3G8fcjW6JQWqh7iy2UXqotLSVg8YotNbW2NgW2SlESwdYiJW40saIxylZjMek/S2pMVfojG4q0kcAfFLQWqBAQibaAd+kCC0tbBKTbolzEtJaa0IWPf8wQLpfdO3PnzMydL7xfyc2dOXN2vk/OzDz3zHfOmU1VIUlqzw+tdwBJ0mgscElqlAUuSY2ywCWpURa4JDVqwzQH27hxY83Pz09zSElq3vbt2x+tqrmVy6da4PPz8ywuLk5zSElqXpL/2Ntyp1AkqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRUz0TUxpkfsvV6zLug1s3rcu4UhfugUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY0aWOBJLk7ySJKdy5Z9LMm9Se5MclWSgycbU5K00jB74JcAJ61Ydj1wTFUdC3wd+PCYc0mSBhhY4FV1M/DYimXXVdWe/tVbgMMnkE2StIpxzIF/ELh2DPcjSVqDTgWe5E+APcClq6yzOcliksWlpaUuw0mSlhm5wJOcBbwHeF9V1b7Wq6ptVbVQVQtzc3OjDidJWmGk/5EnyUnAecDPVdX3xxtJkjSMYQ4jvAz4CnBUkt1JzgEuBA4Crk+yI8mnJ5xTkrTCwD3wqjpzL4s/M4EskqQ18ExMSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1sMCTXJzkkSQ7ly17WZLrk3yj//uQycaUJK00zB74JcBJK5ZtAW6oqtcCN/SvS5KmaGCBV9XNwGMrFp8KfLZ/+bPAe8ecS5I0wKhz4IdW1cMA/d8vH18kSdIwJv4hZpLNSRaTLC4tLU16OEl6wRi1wP8rySsA+r8f2deKVbWtqhaqamFubm7E4SRJK41a4F8AzupfPgv4+/HEkSQNa5jDCC8DvgIclWR3knOArcC7knwDeFf/uiRpijYMWqGqztzHTe8ccxZJ0hp4JqYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjepU4El+P8ndSXYmuSzJD48rmCRpdSMXeJLDgN8FFqrqGGA/4IxxBZMkra7rFMoG4EeSbAAOAL7dPZIkaRgjF3hVfQv4c+Ah4GHgO1V13cr1kmxOsphkcWlpafSkkqRn6TKFcghwKnAk8ErgwCTvX7leVW2rqoWqWpibmxs9qSTpWbpMofwC8EBVLVXVD4ArgRPHE0uSNEiXAn8IeHOSA5IEeCewazyxJEmDdJkDvxW4ArgduKt/X9vGlEuSNMCGLv+4qs4Hzh9TFknSGngmpiQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGdTqVXpM1v+Xq9Y7wgrGe2/rBrZvWbWy1zT1wSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDWqU4EnOTjJFUnuTbIryVvGFUyStLqu30b4l8CXqur0JPsDB4whkyRpCCMXeJKXAG8DfgOgqp4AnhhPLEnSIF32wF8DLAF/k+Q4YDtwblU9vnylJJuBzQBHHHFEh+EkqZvn2/e+d5kD3wAcD3yqqt4EPA5sWblSVW2rqoWqWpibm+swnCRpuS4FvhvYXVW39q9fQa/QJUlTMHKBV9V/At9MclR/0TuBe8aSSpI0UNejUD4EXNo/AuV+4OzukSRJw+hU4FW1A1gYUxZJ0hp4JqYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5Jjepc4En2S/LVJF8cRyBJ0nDGsQd+LrBrDPcjSVqDTgWe5HBgE3DReOJIkobVdQ/848AfAU/ta4Ukm5MsJllcWlrqOJwk6WkjF3iS9wCPVNX21darqm1VtVBVC3Nzc6MOJ0laocse+FuBX0ryIHA58I4kfzuWVJKkgUYu8Kr6cFUdXlXzwBnAjVX1/rElkyStyuPAJalRG8ZxJ1V1E3DTOO5LkjQc98AlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNWosp9JPw/yWq9dt7Ae3blq3sfX8t17PbZ/X7XMPXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNGrnAk7wqyT8l2ZXk7iTnjjOYJGl1Xb6NcA/wB1V1e5KDgO1Jrq+qe8aUTZK0ipH3wKvq4aq6vX/5f4FdwGHjCiZJWt1Y5sCTzANvAm7dy22bkywmWVxaWhrHcJIkxlDgSV4MfB74var67srbq2pbVS1U1cLc3FzX4SRJfZ0KPMmL6JX3pVV15XgiSZKG0eUolACfAXZV1V+ML5IkaRhd9sDfCvw68I4kO/o/p4wplyRpgJEPI6yqfwEyxiySpDXwTExJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjeryP/K8YMxvuXq9I0hjt57P6we3blq3sZ9P3AOXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqM6FXiSk5J8Lcl9SbaMK5QkabCRCzzJfsAngJOBo4Ezkxw9rmCSpNV12QM/Abivqu6vqieAy4FTxxNLkjRIl+8DPwz45rLru4GfWblSks3A5v7V7yX52oD73Qg82iHXpMxqLjDbqMw2ms7ZcsGYkjzXzG63XNAp26v3trBLgWcvy+o5C6q2AduGvtNksaoWOuSaiFnNBWYbldlGY7bRTCJblymU3cCrll0/HPh2tziSpGF1KfB/A16b5Mgk+wNnAF8YTyxJ0iAjT6FU1Z4kvwP8I7AfcHFV3T2GTENPt0zZrOYCs43KbKMx22jGni1Vz5m2liQ1wDMxJalRFrgkNWpdCnzQKfhJ3pfkzv7Pl5McN0PZTu3n2pFkMcnPzkq2Zev9dJInk5w+K9mSvD3Jd/rbbUeSj8xKtmX5diS5O8k/z0q2JH+4bJvt7D+uL5uBXC9N8g9J7uhvs7MnnWkN2Q5JclX/dXpbkmOmmO3iJI8k2bmP25Pkr/rZ70xyfKcBq2qqP/Q+8Px34DXA/sAdwNEr1jkROKR/+WTg1hnK9mKe+ezgWODeWcm2bL0bgWuA02clG/B24Isz+nw7GLgHOKJ//eWzkm3F+r8I3DgLuYA/Bi7oX54DHgP2n5FsHwPO719+PXDDFJ9vbwOOB3bu4/ZTgGvpnUfz5q7dth574ANPwa+qL1fV//Sv3kLvGPNZyfa96j8SwIHs5eSl9crW9yHg88AjU8q1lmzrYZhsvwZcWVUPAVTVtLbdWrfbmcBlM5KrgIOShN5OzWPAnhnJdjRwA0BV3QvMJzl0CtmoqpvpbYt9ORX4XPXcAhyc5BWjjrceBb63U/APW2X9c+j9xZqGobIlOS3JvcDVwAdnJVuSw4DTgE9PKdPThn1M39J/y31tkjdOJ9pQ2V4HHJLkpiTbk3xghrIBkOQA4CR6f5xnIdeFwBvonbx3F3BuVT01I9nuAH4ZIMkJ9E5Dn9ZO4CBr7b9VrUeBD3UKPkCSn6dX4OdNNNGyIfeybG9fD3BVVb0eeC/w0Ymn6hkm28eB86rqySnkWW6YbLcDr66q44C/Bv5u4ql6hsm2AfgpYBPwbuBPk7xu0sFYw2uB3vTJv1bVant34zJMrncDO4BXAj8JXJjkJZMOxnDZttL7g7yD3jvSrzKddwfDWMtjPlCX70IZ1VCn4Cc5FrgIOLmq/nuWsj2tqm5O8uNJNlbVpL9AZ5hsC8DlvXe1bAROSbKnqiZdlgOzVdV3l12+JsknZ2i77QYerarHgceT3AwcB3x9BrI97QymM30Cw+U6G9jan068L8kD9Oabb1vvbP3n2tnQ+9AQeKD/MwvG+xUk05rcXzaJvwG4HziSZz6EeOOKdY4A7gNOnMFsP8EzH2IeD3zr6evrnW3F+pcwvQ8xh9luP7Zsu50APDQr243eVMAN/XUPAHYCx8xCtv56L6U3r3rgDD2enwL+rH/50P7rYOOMZDuY/geqwG/Sm3Oe+HZbNv48+/4QcxPP/hDzti5jTX0PvPZxCn6S3+rf/mngI8CPAp/s703uqSl8w9iQ2X4F+ECSHwD/B/xq9R+ZGci2LobMdjrw20n20NtuZ8zKdquqXUm+BNwJPAVcVFV7PQxs2tn6q54GXFe9dwgTN2SujwKXJLmLXhmdV5N/NzVstjcAn0vyJL2ji86ZdK6nJbmM3hFXG5PsBs4HXrQs2zX0jkS5D/g+/XcKI483hdeQJGkCPBNTkhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RG/T/8+pKEQvrlUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram the out of genre scores\n",
    "\n",
    "plt.hist(out_of_genre_scores[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From just looking at the histogram, things don't look great. The out of sample score distribution looks a kind of similar to the test score distribution so it may be harder to distinguish the Fantasy genre than we thought.\n",
    "\n",
    "Let's check by adding these scores back into the out of sample dataframe we created, and taking a look at the high scoring rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the scores as a 'score' column to the out_of_sample df\n",
    "\n",
    "out_of_genre['score'] = out_of_genre_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the df by your score column, from highest value to lowest\n",
    "\n",
    "try_it_out = out_of_genre.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Les Fleurs du Mal</td>\n",
       "      <td>Ã‚Â£29.04</td>\n",
       "      <td>Presents the first American translation of the...</td>\n",
       "      <td>Five</td>\n",
       "      <td>les-fleurs-du-mal_530/index.html</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>Misery</td>\n",
       "      <td>Ã‚Â£34.79</td>\n",
       "      <td>Alternate cover edition here.Paul Sheldon. He'...</td>\n",
       "      <td>Two</td>\n",
       "      <td>misery_332/index.html</td>\n",
       "      <td>Horror</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>A la Mode: 120 Recipes in 60 Pairings: Pies, T...</td>\n",
       "      <td>Ã‚Â£38.77</td>\n",
       "      <td>Are you ready to take your baking over the top...</td>\n",
       "      <td>One</td>\n",
       "      <td>a-la-mode-120-recipes-in-60-pairings-pies-tart...</td>\n",
       "      <td>Food and Drink</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Obsidian (Lux #1)</td>\n",
       "      <td>Ã‚Â£14.86</td>\n",
       "      <td>Starting over sucks.When we moved to West Virg...</td>\n",
       "      <td>Two</td>\n",
       "      <td>obsidian-lux-1_911/index.html</td>\n",
       "      <td>Young Adult</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>Giant Days, Vol. 1 (Giant Days #1-4)</td>\n",
       "      <td>Ã‚Â£56.76</td>\n",
       "      <td>Susan, Esther, and Daisy started at university...</td>\n",
       "      <td>Four</td>\n",
       "      <td>giant-days-vol-1-giant-days-1-4_22/index.html</td>\n",
       "      <td>Sequential Art</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>Foundation (Foundation (Publication Order) #1)</td>\n",
       "      <td>Ã‚Â£32.42</td>\n",
       "      <td>For twelve thousand years the Galactic Empire ...</td>\n",
       "      <td>One</td>\n",
       "      <td>foundation-foundation-publication-order-1_375/...</td>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>The Mirror &amp; the Maze (The Wrath and the Dawn ...</td>\n",
       "      <td>Ã‚Â£29.38</td>\n",
       "      <td>The city of Rey is burning. With smoke billowi...</td>\n",
       "      <td>One</td>\n",
       "      <td>the-mirror-the-maze-the-wrath-and-the-dawn-15_...</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>0.770501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>It Didn't Start with You: How Inherited Family...</td>\n",
       "      <td>Ã‚Â£56.27</td>\n",
       "      <td>A groundbreaking approach to transforming trau...</td>\n",
       "      <td>Three</td>\n",
       "      <td>it-didnt-start-with-you-how-inherited-family-t...</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>0.722185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Letter to a Christian Nation</td>\n",
       "      <td>Ã‚Â£22.20</td>\n",
       "      <td>In response to The End of Faith, Sam Harris re...</td>\n",
       "      <td>One</td>\n",
       "      <td>letter-to-a-christian-nation_186/index.html</td>\n",
       "      <td>Nonfiction</td>\n",
       "      <td>0.722185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>How to Be a Domestic Goddess: Baking and the A...</td>\n",
       "      <td>Ã‚Â£28.25</td>\n",
       "      <td>Nigella Lawson's How to Be a Domestic Goddess ...</td>\n",
       "      <td>Two</td>\n",
       "      <td>how-to-be-a-domestic-goddess-baking-and-the-ar...</td>\n",
       "      <td>Food and Drink</td>\n",
       "      <td>0.698027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title    Price  \\\n",
       "470                                  Les Fleurs du Mal  Ã‚Â£29.04   \n",
       "668                                             Misery  Ã‚Â£34.79   \n",
       "389  A la Mode: 120 Recipes in 60 Pairings: Pies, T...  Ã‚Â£38.77   \n",
       "89                                   Obsidian (Lux #1)  Ã‚Â£14.86   \n",
       "978               Giant Days, Vol. 1 (Giant Days #1-4)  Ã‚Â£56.76   \n",
       "625     Foundation (Foundation (Publication Order) #1)  Ã‚Â£32.42   \n",
       "927  The Mirror & the Maze (The Wrath and the Dawn ...  Ã‚Â£29.38   \n",
       "407  It Didn't Start with You: How Inherited Family...  Ã‚Â£56.27   \n",
       "814                       Letter to a Christian Nation  Ã‚Â£22.20   \n",
       "530  How to Be a Domestic Goddess: Baking and the A...  Ã‚Â£28.25   \n",
       "\n",
       "                                           Description Rating  \\\n",
       "470  Presents the first American translation of the...   Five   \n",
       "668  Alternate cover edition here.Paul Sheldon. He'...    Two   \n",
       "389  Are you ready to take your baking over the top...    One   \n",
       "89   Starting over sucks.When we moved to West Virg...    Two   \n",
       "978  Susan, Esther, and Daisy started at university...   Four   \n",
       "625  For twelve thousand years the Galactic Empire ...    One   \n",
       "927  The city of Rey is burning. With smoke billowi...    One   \n",
       "407  A groundbreaking approach to transforming trau...  Three   \n",
       "814  In response to The End of Faith, Sam Harris re...    One   \n",
       "530  Nigella Lawson's How to Be a Domestic Goddess ...    Two   \n",
       "\n",
       "                                                  Link            Genre  \\\n",
       "470                   les-fleurs-du-mal_530/index.html           Poetry   \n",
       "668                              misery_332/index.html           Horror   \n",
       "389  a-la-mode-120-recipes-in-60-pairings-pies-tart...   Food and Drink   \n",
       "89                       obsidian-lux-1_911/index.html      Young Adult   \n",
       "978      giant-days-vol-1-giant-days-1-4_22/index.html   Sequential Art   \n",
       "625  foundation-foundation-publication-order-1_375/...  Science Fiction   \n",
       "927  the-mirror-the-maze-the-wrath-and-the-dawn-15_...          Fantasy   \n",
       "407  it-didnt-start-with-you-how-inherited-family-t...       Psychology   \n",
       "814        letter-to-a-christian-nation_186/index.html       Nonfiction   \n",
       "530  how-to-be-a-domestic-goddess-baking-and-the-ar...   Food and Drink   \n",
       "\n",
       "        score  \n",
       "470  1.000000  \n",
       "668  1.000000  \n",
       "389  1.000000  \n",
       "89   1.000000  \n",
       "978  1.000000  \n",
       "625  1.000000  \n",
       "927  0.770501  \n",
       "407  0.722185  \n",
       "814  0.722185  \n",
       "530  0.698027  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first 10 rows of the df\n",
    "\n",
    "try_it_out.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fantasy               5\n",
       "Young Adult           2\n",
       "Food and Drink        2\n",
       "Sports and Games      1\n",
       "Autobiography         1\n",
       "Sequential Art        1\n",
       "Historical Fiction    1\n",
       "Christian             1\n",
       "Science Fiction       1\n",
       "Nonfiction            1\n",
       "History               1\n",
       "Horror                1\n",
       "Psychology            1\n",
       "Poetry                1\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the Genre value counts of the 20 highest scoring books\n",
    "\n",
    "try_it_out.head(20).Genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons learned:\n",
    "\n",
    "If you look at the dataframe above, you'll see our scoring method performed very poorly. This is for a number of reasons:\n",
    "\n",
    "- The vectorization we used is extremely simplistic and just uses word counts. In a real world application, a data scientist would use a far more advanced embedding method.\n",
    "- Our dataset is fairly small, and it seems like the Fantasy genre doesn't have many unique words like we'd hoped.\n",
    "    - We can tell this by looking at the top 10 tokens in the training set. They don't seem particularly 'fantasy-like' to me:\n",
    "        - ['book', 'find', 'four', 'life', 'new', 'one', 'power', 'series', 'seven', 'world']\n",
    "- There's clearly a high amount of linguistic overlap between Fantasy, Non-Fiction, Young Adult, and Sequential Art book descriptions. You can tell this by looking at the highest scoring books in our out of sample test we ran. 14 out of the top 20 scoring books fall into one of those genres.\n",
    "\n",
    "### So - how do we improve on this?\n",
    "- Experiment with different numbers of features to look for. We started with 10, but you should expreriment toggling it up and down. Keep in mind that when you add tokens, even though it might make finding the genre easier, you also run the risk of catching other erroneous genres. This is a double edged sword.\n",
    "- Turn the above steps into a sklearn pipeline so that you can easily run a few dozen attempts, and so that the data scientist you're likely building this pipeline for can easily grid search on it.\n",
    "- Use a better vectorization method. There are lot's of better ways to do this that are more likely to be used in the real world:\n",
    "    - TF-IDF stands for Term Frequency - Inverse Document Frequency. This method lowers the value given to a word if it appears many times in the overall dataset. So for example, in our book descriptions we're looking at we're specifically looking for words that are common to *only* the Fantasy genre. Words that are generally common like 'book' or aren't particularly helpful.\n",
    "    \n",
    "    - Use something more advanced than word frequency based methods\n",
    "\n",
    "- Lemmatize the words. This is cutting the words down to just their stems so that words with similar roots are grouped together.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
